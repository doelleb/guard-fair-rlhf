# GUARD: Guiding Unbiased Alignment through Reward Debiasing
Reward misspecification in RLHF leads models to optimize unintended objectives and degrade alignment with user values and preferences, yet expert-defined harm labels act as a stable human intervening signal for post-training by setting a fairness standard, and preventing drift. In this paper, we address the problem of categorical bias in reward model training through a mutual-information minimization constraint alongside an intrinsic reward in PPO to preserve output diversity along these categories. Empirically, our Fair-RM achieves near-neutral bias on CrowS-Pairs and StereoSet, while improving alignment loss. We further demonstrate scalability to multiclass fairness across 19 safety categories in the PKU-SafeRLHF dataset, substantially reducing reward disparities. We conclude that mutual-informationâ€“based fairness constraints, grounded in human-defined categories, offer a general solution for bias mitigation in RLHF reward models.
This repository contains all of the code for our experiments.
